"""
å¤‡ä»½ç®¡ç† API
æä¾›å®Œæ•´çš„ç³»ç»Ÿå¤‡ä»½ã€æ¢å¤ã€ä¸‹è½½ã€å‡çº§åŠŸèƒ½
æ‰€æœ‰æ“ä½œé€šè¿‡ Web ç•Œé¢å®Œæˆ,æ— éœ€ SSH ç™»å½•æœåŠ¡å™¨
"""
from fastapi import APIRouter, HTTPException, BackgroundTasks
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import List, Optional
import os
import sys
import datetime
import shutil
import tarfile
import json
import subprocess
from pathlib import Path

router = APIRouter(tags=["å¤‡ä»½ç®¡ç†"])

# é…ç½® - ä»ç¯å¢ƒå˜é‡è¯»å–
# è·å–é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

BACKUP_DIR = os.getenv("BACKUP_DIR", os.path.join(PROJECT_ROOT, "backups"))
DATA_DIR = os.getenv("DATA_DIR", os.path.join(PROJECT_ROOT, "backend", "data"))

# æ•°æ®åº“è·¯å¾„è‡ªåŠ¨æ£€æµ‹
db_path_env = os.getenv("DATABASE_URL", f"{DATA_DIR}/el.db").replace("sqlite:///", "")
if not os.path.exists(db_path_env):
    # å°è¯• backend/el.db
    alt_db_path = os.path.join(PROJECT_ROOT, "backend", "el.db")
    if os.path.exists(alt_db_path):
        DB_PATH = alt_db_path
        DATA_DIR = os.path.join(PROJECT_ROOT, "backend")
    else:
        DB_PATH = db_path_env
else:
    DB_PATH = db_path_env

# åª’ä½“æ–‡ä»¶è·¯å¾„è‡ªåŠ¨æ£€æµ‹
media_dir_env = os.getenv("MEDIA_DIR", f"{DATA_DIR}/media")
if not os.path.exists(media_dir_env):
    # å°è¯• backend/media
    alt_media_dir = os.path.join(PROJECT_ROOT, "backend", "media")
    if os.path.exists(alt_media_dir):
        MEDIA_DIR = alt_media_dir
    else:
        MEDIA_DIR = media_dir_env
else:
    MEDIA_DIR = media_dir_env

APP_DIR = os.getenv("APP_DIR", PROJECT_ROOT)

# å¤‡ä»½é…ç½®æ–‡ä»¶è·¯å¾„
BACKUP_CONFIG_FILE = os.path.join(DATA_DIR, "backup_config.json")


class BackupInfo(BaseModel):
    """å¤‡ä»½ä¿¡æ¯"""
    filename: str
    size: int
    size_human: str
    created_at: str
    description: Optional[str] = None


class BackupCreateRequest(BaseModel):
    """åˆ›å»ºå¤‡ä»½è¯·æ±‚ - ç®€åŒ–ç‰ˆ,ç»Ÿä¸€æ‰“åŒ…"""
    description: Optional[str] = None


class RestoreRequest(BaseModel):
    """æ¢å¤å¤‡ä»½è¯·æ±‚ - ç®€åŒ–ç‰ˆ,å®Œæ•´æ¢å¤"""
    filename: str


class BackupConfig(BaseModel):
    """å¤‡ä»½é…ç½®"""
    auto_backup_enabled: bool = True
    auto_backup_time: str = "02:00"  # æ¯å¤©å‡Œæ™¨2ç‚¹
    backup_retention_days: int = 30  # ä¿ç•™30å¤©
    auto_cleanup_enabled: bool = True


class SystemSettings(BaseModel):
    """ç³»ç»Ÿè®¾ç½®"""
    backup_config: BackupConfig
    system_info: dict


def get_human_size(size_bytes: int) -> str:
    """è½¬æ¢æ–‡ä»¶å¤§å°ä¸ºäººç±»å¯è¯»æ ¼å¼"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.2f} TB"


def ensure_backup_dir():
    """ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨"""
    os.makedirs(BACKUP_DIR, exist_ok=True)


def load_backup_config() -> BackupConfig:
    """åŠ è½½å¤‡ä»½é…ç½®"""
    if os.path.exists(BACKUP_CONFIG_FILE):
        try:
            with open(BACKUP_CONFIG_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return BackupConfig(**data)
        except:
            pass
    return BackupConfig()


def save_backup_config(config: BackupConfig):
    """ä¿å­˜å¤‡ä»½é…ç½®"""
    os.makedirs(os.path.dirname(BACKUP_CONFIG_FILE), exist_ok=True)
    with open(BACKUP_CONFIG_FILE, 'w', encoding='utf-8') as f:
        json.dump(config.dict(), f, indent=2, ensure_ascii=False)


def parse_env_file(filepath: str) -> dict:
    """è§£æ .env æ–‡ä»¶ï¼Œè¿”å› {key: value} å­—å…¸"""
    result = {}
    if not os.path.exists(filepath):
        return result
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                if not line or line.startswith('#'):
                    continue
                # è§£æ KEY=VALUE
                if '=' in line:
                    key, _, value = line.partition('=')
                    key = key.strip()
                    value = value.strip()
                    # ç§»é™¤å¼•å·
                    if value and value[0] in ('"', "'") and value[-1] == value[0]:
                        value = value[1:-1]
                    result[key] = value
    except Exception:
        pass
    return result


def check_and_merge_env_config(app_dir: str) -> dict:
    """
    æ£€æŸ¥å¹¶åˆå¹¶ç¯å¢ƒé…ç½®

    æ¯”è¾ƒ .env.example å’Œå½“å‰ .envï¼ˆæˆ– /etc/englishlearn.envï¼‰ï¼Œ
    å°†ç¼ºå¤±çš„é…ç½®é¡¹æ·»åŠ åˆ°ç”¨æˆ·çš„ .env æ–‡ä»¶ä¸­ã€‚

    è¿”å›: {"new_keys": [...], "env_file": "..."}
    """
    result = {"new_keys": [], "env_file": None}

    # æŸ¥æ‰¾ .env.example
    example_path = os.path.join(app_dir, ".env.example")
    if not os.path.exists(example_path):
        return result

    # æŸ¥æ‰¾ç”¨æˆ·çš„ .env æ–‡ä»¶
    # ä¼˜å…ˆä½¿ç”¨ /etc/englishlearn.envï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
    env_paths = [
        "/etc/englishlearn.env",
        os.path.join(app_dir, ".env"),
    ]

    user_env_path = None
    for path in env_paths:
        if os.path.exists(path):
            user_env_path = path
            break

    if not user_env_path:
        # æ²¡æœ‰ç°æœ‰é…ç½®æ–‡ä»¶ï¼Œä¸åšå¤„ç†
        return result

    result["env_file"] = user_env_path

    # è§£æé…ç½®æ–‡ä»¶
    example_config = parse_env_file(example_path)
    user_config = parse_env_file(user_env_path)

    # æ‰¾å‡ºç¼ºå¤±çš„é…ç½®é¡¹
    missing_keys = []
    for key in example_config:
        if key not in user_config:
            missing_keys.append(key)

    if not missing_keys:
        return result

    # è¯»å–åŸå§‹ .env.example æ–‡ä»¶å†…å®¹ï¼Œä¿ç•™æ³¨é‡Šç»“æ„
    try:
        with open(example_path, 'r', encoding='utf-8') as f:
            example_lines = f.readlines()
    except Exception:
        return result

    # æ„å»ºè¦æ·»åŠ çš„å†…å®¹å—
    # æŒ‰ç…§ .env.example çš„é¡ºåºï¼Œæå–ç¼ºå¤±é…ç½®é¡¹åŠå…¶å‰é¢çš„æ³¨é‡Š
    lines_to_add = []
    current_comments = []
    added_keys = set()

    for line in example_lines:
        stripped = line.strip()
        if not stripped or stripped.startswith('#'):
            current_comments.append(line)
        elif '=' in stripped:
            key = stripped.partition('=')[0].strip()
            if key in missing_keys and key not in added_keys:
                # æ·»åŠ æ³¨é‡Šå’Œé…ç½®è¡Œ
                if current_comments:
                    lines_to_add.extend(current_comments)
                lines_to_add.append(line)
                added_keys.add(key)
            current_comments = []
        else:
            current_comments = []

    if not lines_to_add:
        return result

    # è¿½åŠ åˆ°ç”¨æˆ·çš„ .env æ–‡ä»¶
    try:
        with open(user_env_path, 'a', encoding='utf-8') as f:
            f.write("\n\n# ============================================================\n")
            f.write(f"# ä»¥ä¸‹é…ç½®ç”±ç³»ç»Ÿå‡çº§è‡ªåŠ¨æ·»åŠ  ({datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\n")
            f.write("# ============================================================\n")
            f.writelines(lines_to_add)

        result["new_keys"] = list(added_keys)
    except Exception as e:
        # å†™å…¥å¤±è´¥ï¼Œè®°å½•ä½†ä¸ä¸­æ–­å‡çº§
        pass

    return result


@router.get("/list", response_model=List[BackupInfo])
async def list_backups():
    """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½"""
    ensure_backup_dir()

    backups = []
    for filename in sorted(os.listdir(BACKUP_DIR), reverse=True):
        if not filename.endswith('.tar.gz'):
            continue

        filepath = os.path.join(BACKUP_DIR, filename)
        stat = os.stat(filepath)

        # å°è¯•è¯»å–å¤‡ä»½æè¿°
        description = None
        try:
            with tarfile.open(filepath, 'r:gz') as tar:
                if 'backup_info.json' in tar.getnames():
                    info_file = tar.extractfile('backup_info.json')
                    if info_file:
                        info_data = json.loads(info_file.read().decode('utf-8'))
                        description = info_data.get('description')
        except:
            pass

        backups.append(BackupInfo(
            filename=filename,
            size=stat.st_size,
            size_human=get_human_size(stat.st_size),
            created_at=datetime.datetime.fromtimestamp(stat.st_mtime).isoformat(),
            description=description
        ))

    return backups


@router.post("/create")
async def create_backup(request: BackupCreateRequest):
    """
    åˆ›å»ºç³»ç»Ÿå¤‡ä»½

    å¤‡ä»½å†…å®¹åŒ…æ‹¬:
    - æ•°æ®åº“æ–‡ä»¶ (el.db)
    - æ‰€æœ‰åª’ä½“æ–‡ä»¶ (media/)
    - å¤‡ä»½ä¿¡æ¯ (backup_info.json)

    ç”¨æˆ·æ— éœ€å…³å¿ƒå…·ä½“åŒ…å«ä»€ä¹ˆ,è¿™ä¸ªå¤‡ä»½åŒ…å¯ä»¥æ¢å¤ç³»ç»Ÿåˆ°å¤‡ä»½æ—¶åˆ»çš„çŠ¶æ€
    """
    ensure_backup_dir()

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"EnglishLearn_å¤‡ä»½_{timestamp}.tar.gz"
    filepath = os.path.join(BACKUP_DIR, filename)

    try:
        # æ£€æŸ¥ .env æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        env_path = os.path.join(PROJECT_ROOT, ".env")
        env_exists = os.path.exists(env_path)

        # åˆ›å»ºå¤‡ä»½ä¿¡æ¯
        backup_info = {
            "backup_time": datetime.datetime.now().isoformat(),
            "description": request.description or f"ç³»ç»Ÿå¤‡ä»½ - {timestamp}",
            "version": "1.0",
            "db_included": True,
            "media_included": True,
            "env_included": env_exists,  # æ ¹æ®å®é™…æƒ…å†µè®¾ç½®
            "restore_note": "æ­¤å¤‡ä»½åŒ…å«æ‰€æœ‰ç³»ç»Ÿæ•°æ®ï¼Œå¯æ¢å¤åˆ°å¤‡ä»½æ—¶åˆ»çš„çŠ¶æ€"
        }

        # ç›´æ¥åˆ›å»º tar.gz å‹ç¼©åŒ…
        with tarfile.open(filepath, "w:gz") as tar:
            # æ·»åŠ å¤‡ä»½ä¿¡æ¯æ–‡ä»¶
            info_json = json.dumps(backup_info, indent=2, ensure_ascii=False)
            info_bytes = info_json.encode('utf-8')

            import io
            info_tarinfo = tarfile.TarInfo(name='backup_info.json')
            info_tarinfo.size = len(info_bytes)
            info_tarinfo.mtime = datetime.datetime.now().timestamp()
            tar.addfile(info_tarinfo, io.BytesIO(info_bytes))

            # æ·»åŠ æ•°æ®åº“æ–‡ä»¶
            if os.path.exists(DB_PATH):
                tar.add(DB_PATH, arcname='el.db')

            # æ·»åŠ åª’ä½“æ–‡ä»¶ç›®å½•
            if os.path.exists(MEDIA_DIR):
                tar.add(MEDIA_DIR, arcname='media')

            # æ·»åŠ  .env æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if env_exists:
                tar.add(env_path, arcname='.env')

        # è·å–æ–‡ä»¶å¤§å°
        size = os.path.getsize(filepath)

        return {
            "success": True,
            "message": "å¤‡ä»½åˆ›å»ºæˆåŠŸ!",
            "filename": filename,
            "size": size,
            "size_human": get_human_size(size),
            "note": "æ­¤å¤‡ä»½åŒ…å«å®Œæ•´çš„ç³»ç»Ÿæ•°æ®,å¯ç”¨äºæ¢å¤åˆ°å¤‡ä»½æ—¶åˆ»çš„å®Œæ•´çŠ¶æ€"
        }

    except Exception as e:
        # æ¸…ç†å¤±è´¥çš„å¤‡ä»½æ–‡ä»¶
        if os.path.exists(filepath):
            os.remove(filepath)
        raise HTTPException(status_code=500, detail=f"å¤‡ä»½å¤±è´¥: {str(e)}")


@router.get("/download/{filename}")
async def download_backup(filename: str):
    """ä¸‹è½½å¤‡ä»½æ–‡ä»¶åˆ°æœ¬åœ°"""
    filepath = os.path.join(BACKUP_DIR, filename)

    if not os.path.exists(filepath):
        raise HTTPException(status_code=404, detail="å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨")

    if not filename.endswith('.tar.gz'):
        raise HTTPException(status_code=400, detail="æ— æ•ˆçš„å¤‡ä»½æ–‡ä»¶")

    return FileResponse(
        filepath,
        media_type='application/gzip',
        filename=filename,
        headers={
            "Content-Disposition": f'attachment; filename="{filename}"'
        }
    )


@router.post("/restore")
async def restore_backup(request: RestoreRequest):
    """
    æ¢å¤ç³»ç»Ÿå¤‡ä»½

    è­¦å‘Š: æ­¤æ“ä½œå°†è¦†ç›–å½“å‰æ‰€æœ‰æ•°æ®!
    å»ºè®®åœ¨æ¢å¤å‰å…ˆåˆ›å»ºå½“å‰çŠ¶æ€çš„å¤‡ä»½

    æ¢å¤è¿‡ç¨‹:
    1. è§£å‹å¤‡ä»½åŒ…
    2. æ¢å¤æ•°æ®åº“
    3. æ¢å¤åª’ä½“æ–‡ä»¶
    4. ç³»ç»Ÿå°†è‡ªåŠ¨é‡å¯ä»¥åŠ è½½æ–°æ•°æ®
    """
    filepath = os.path.join(BACKUP_DIR, request.filename)

    if not os.path.exists(filepath):
        raise HTTPException(status_code=404, detail="å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨")

    try:
        # åˆ›å»ºä¸´æ—¶è§£å‹ç›®å½•
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        temp_dir = os.path.join(BACKUP_DIR, f"restore_temp_{timestamp}")
        os.makedirs(temp_dir, exist_ok=True)

        # è§£å‹å¤‡ä»½æ–‡ä»¶
        with tarfile.open(filepath, "r:gz") as tar:
            tar.extractall(temp_dir)

        # å¤‡ä»½å½“å‰æ•°æ® (ä»¥é˜²æ¢å¤å¤±è´¥)
        if os.path.exists(DB_PATH):
            backup_current_db = f"{DB_PATH}.before_restore_{timestamp}"
            shutil.copy2(DB_PATH, backup_current_db)

        if os.path.exists(MEDIA_DIR):
            backup_current_media = f"{MEDIA_DIR}_before_restore_{timestamp}"
            shutil.copytree(MEDIA_DIR, backup_current_media, dirs_exist_ok=True)

        # æ¢å¤æ•°æ®åº“
        db_file = os.path.join(temp_dir, "el.db")
        if os.path.exists(db_file):
            shutil.copy2(db_file, DB_PATH)
        else:
            raise Exception("å¤‡ä»½æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æ•°æ®åº“")

        # æ¢å¤åª’ä½“æ–‡ä»¶
        media_dir = os.path.join(temp_dir, "media")
        if os.path.exists(media_dir):
            # åˆ é™¤æ—§åª’ä½“ç›®å½•
            if os.path.exists(MEDIA_DIR):
                shutil.rmtree(MEDIA_DIR)
            # å¤åˆ¶æ–°åª’ä½“ç›®å½•
            shutil.copytree(media_dir, MEDIA_DIR)

        # æ¢å¤ .env æ–‡ä»¶ï¼ˆå¦‚æœå¤‡ä»½ä¸­åŒ…å«ï¼‰
        env_file = os.path.join(temp_dir, ".env")
        if os.path.exists(env_file):
            env_dest = os.path.join(PROJECT_ROOT, ".env")
            # å¤‡ä»½å½“å‰ .envï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if os.path.exists(env_dest):
                env_backup = f"{env_dest}.before_restore_{timestamp}"
                shutil.copy2(env_dest, env_backup)
            # æ¢å¤ .env
            shutil.copy2(env_file, env_dest)

        # æ¸…ç†ä¸´æ—¶ç›®å½•
        shutil.rmtree(temp_dir)

        # æ£€æŸ¥å¹¶è¿è¡Œæ•°æ®åº“è¿ç§»ï¼ˆæ”¯æŒä»è€ç‰ˆæœ¬æ¢å¤ï¼‰
        migration_result = None
        try:
            # å¯¼å…¥è¿ç§»ç®¡ç†å™¨
            migration_manager_path = os.path.join(PROJECT_ROOT, "backend", "migration_manager.py")
            if os.path.exists(migration_manager_path):
                # ä½¿ç”¨ subprocess è¿è¡Œè¿ç§»ï¼Œé¿å…å¯¼å…¥é—®é¢˜
                result = subprocess.run(
                    [sys.executable, migration_manager_path, "migrate"],
                    cwd=PROJECT_ROOT,
                    capture_output=True,
                    text=True,
                    timeout=60
                )

                if result.returncode == 0:
                    migration_result = {
                        "success": True,
                        "message": "æ•°æ®åº“è¿ç§»æˆåŠŸ",
                        "output": result.stdout
                    }
                else:
                    migration_result = {
                        "success": False,
                        "message": "æ•°æ®åº“è¿ç§»å¤±è´¥ï¼Œä½†æ•°æ®å·²æ¢å¤",
                        "error": result.stderr
                    }
        except Exception as e:
            migration_result = {
                "success": False,
                "message": f"æ— æ³•è¿è¡Œæ•°æ®åº“è¿ç§»: {str(e)}"
            }

        # è¯»å–å¤‡ä»½ä¿¡æ¯
        backup_info = None
        env_restored = False
        try:
            with tarfile.open(filepath, 'r:gz') as tar:
                members = tar.getnames()
                if 'backup_info.json' in members:
                    info_file = tar.extractfile('backup_info.json')
                    if info_file:
                        backup_info = json.loads(info_file.read().decode('utf-8'))
                if '.env' in members:
                    env_restored = True
        except:
            pass

        return {
            "success": True,
            "message": "ç³»ç»Ÿæ¢å¤æˆåŠŸ!",
            "restored_from": request.filename,
            "backup_time": backup_info.get('backup_time') if backup_info else None,
            "env_restored": env_restored,
            "migration_result": migration_result,
            "note": ("æ•°æ®å·²æ¢å¤,å»ºè®®åˆ·æ–°é¡µé¢ä»¥åŠ è½½æœ€æ–°æ•°æ®ã€‚" +
                    ("å¦‚æœæ¢å¤äº† .env æ–‡ä»¶,è¯·é‡å¯æœåŠ¡ä»¥ä½¿é…ç½®ç”Ÿæ•ˆã€‚" if env_restored else "") +
                    (" æ•°æ®åº“è¿ç§»å·²è‡ªåŠ¨è¿è¡Œã€‚" if migration_result and migration_result.get("success") else "")),
            "rollback_files": {
                "db": f"{DB_PATH}.before_restore_{timestamp}",
                "media": f"{MEDIA_DIR}_before_restore_{timestamp}",
                "env": f"{os.path.join(PROJECT_ROOT, '.env')}.before_restore_{timestamp}" if env_restored else None
            }
        }

    except Exception as e:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        raise HTTPException(status_code=500, detail=f"æ¢å¤å¤±è´¥: {str(e)}")


@router.delete("/delete/{filename}")
async def delete_backup(filename: str):
    """åˆ é™¤å¤‡ä»½æ–‡ä»¶"""
    filepath = os.path.join(BACKUP_DIR, filename)

    if not os.path.exists(filepath):
        raise HTTPException(status_code=404, detail="å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨")

    if not filename.endswith('.tar.gz'):
        raise HTTPException(status_code=400, detail="æ— æ•ˆçš„å¤‡ä»½æ–‡ä»¶")

    try:
        os.remove(filepath)
        return {
            "success": True,
            "message": "å¤‡ä»½æ–‡ä»¶å·²åˆ é™¤",
            "filename": filename
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤å¤±è´¥: {str(e)}")


@router.post("/cleanup")
async def cleanup_old_backups(keep_days: int = 30):
    """
    æ¸…ç†æ—§å¤‡ä»½

    é»˜è®¤ä¿ç•™æœ€è¿‘ 30 å¤©çš„å¤‡ä»½,å¯ä»¥åœ¨ç³»ç»Ÿè®¾ç½®ä¸­é…ç½®
    """
    ensure_backup_dir()

    cutoff_time = datetime.datetime.now() - datetime.timedelta(days=keep_days)
    deleted_count = 0
    freed_space = 0

    for filename in os.listdir(BACKUP_DIR):
        if not filename.endswith('.tar.gz'):
            continue

        filepath = os.path.join(BACKUP_DIR, filename)
        stat = os.stat(filepath)
        mtime = datetime.datetime.fromtimestamp(stat.st_mtime)

        if mtime < cutoff_time:
            freed_space += stat.st_size
            os.remove(filepath)
            deleted_count += 1

    return {
        "success": True,
        "message": f"æ¸…ç†å®Œæˆ,åˆ é™¤äº† {deleted_count} ä¸ªæ—§å¤‡ä»½",
        "deleted_count": deleted_count,
        "freed_space": freed_space,
        "freed_space_human": get_human_size(freed_space),
        "keep_days": keep_days
    }


@router.get("/space")
async def get_backup_space():
    """è·å–å¤‡ä»½ç©ºé—´ä½¿ç”¨æƒ…å†µ"""
    ensure_backup_dir()

    total_size = 0
    backup_count = 0

    for filename in os.listdir(BACKUP_DIR):
        if filename.endswith('.tar.gz'):
            filepath = os.path.join(BACKUP_DIR, filename)
            total_size += os.path.getsize(filepath)
            backup_count += 1

    # è·å–ç£ç›˜ç©ºé—´
    stat = os.statvfs(BACKUP_DIR)
    disk_total = stat.f_blocks * stat.f_frsize
    disk_free = stat.f_bavail * stat.f_frsize
    disk_used = disk_total - disk_free

    return {
        "backup_count": backup_count,
        "backup_total_size": total_size,
        "backup_total_size_human": get_human_size(total_size),
        "disk_total": disk_total,
        "disk_total_human": get_human_size(disk_total),
        "disk_used": disk_used,
        "disk_used_human": get_human_size(disk_used),
        "disk_free": disk_free,
        "disk_free_human": get_human_size(disk_free),
        "disk_usage_percent": round(disk_used / disk_total * 100, 2)
    }


@router.get("/config", response_model=BackupConfig)
async def get_backup_config():
    """è·å–å¤‡ä»½é…ç½®"""
    return load_backup_config()


@router.post("/config")
async def update_backup_config(config: BackupConfig):
    """
    æ›´æ–°å¤‡ä»½é…ç½®

    é…ç½®é¡¹:
    - auto_backup_enabled: æ˜¯å¦å¯ç”¨è‡ªåŠ¨å¤‡ä»½
    - auto_backup_time: è‡ªåŠ¨å¤‡ä»½æ—¶é—´ (24å°æ—¶åˆ¶,å¦‚ "02:00")
    - backup_retention_days: å¤‡ä»½ä¿ç•™å¤©æ•°
    - auto_cleanup_enabled: æ˜¯å¦è‡ªåŠ¨æ¸…ç†æ—§å¤‡ä»½
    """
    try:
        save_backup_config(config)

        # è¿™é‡Œåº”è¯¥æ›´æ–° crontab æˆ– systemd timer
        # ç®€åŒ–ç‰ˆæœ¬:ä¿å­˜é…ç½®æ–‡ä»¶,ç”±å¤–éƒ¨è„šæœ¬è¯»å–

        return {
            "success": True,
            "message": "å¤‡ä»½é…ç½®å·²æ›´æ–°",
            "config": config.dict(),
            "note": "é…ç½®å·²ä¿å­˜,è‡ªåŠ¨å¤‡ä»½ä»»åŠ¡å°†åœ¨ä¸‹æ¬¡æ‰§è¡Œæ—¶ç”Ÿæ•ˆ"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é…ç½®æ›´æ–°å¤±è´¥: {str(e)}")


@router.get("/system-info")
async def get_system_info():
    """
    è·å–ç³»ç»Ÿä¿¡æ¯

    ç”¨äºåœ¨ Web ç•Œé¢æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
    """
    import platform
    import psutil

    # CPU ä¿¡æ¯
    cpu_percent = psutil.cpu_percent(interval=1)
    cpu_count = psutil.cpu_count()

    # å†…å­˜ä¿¡æ¯
    memory = psutil.virtual_memory()

    # ç£ç›˜ä¿¡æ¯
    disk = psutil.disk_usage('/')

    # æ•°æ®åº“å¤§å°
    db_size = os.path.getsize(DB_PATH) if os.path.exists(DB_PATH) else 0

    # åª’ä½“æ–‡ä»¶å¤§å°
    media_size = 0
    if os.path.exists(MEDIA_DIR):
        for dirpath, dirnames, filenames in os.walk(MEDIA_DIR):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                media_size += os.path.getsize(filepath)

    return {
        "system": {
            "os": platform.system(),
            "os_version": platform.release(),
            "python_version": platform.python_version(),
            "hostname": platform.node()
        },
        "cpu": {
            "count": cpu_count,
            "usage_percent": cpu_percent
        },
        "memory": {
            "total": memory.total,
            "total_human": get_human_size(memory.total),
            "used": memory.used,
            "used_human": get_human_size(memory.used),
            "usage_percent": memory.percent
        },
        "disk": {
            "total": disk.total,
            "total_human": get_human_size(disk.total),
            "used": disk.used,
            "used_human": get_human_size(disk.used),
            "free": disk.free,
            "free_human": get_human_size(disk.free),
            "usage_percent": disk.percent
        },
        "data": {
            "database_size": db_size,
            "database_size_human": get_human_size(db_size),
            "media_size": media_size,
            "media_size_human": get_human_size(media_size),
            "total_data_size": db_size + media_size,
            "total_data_size_human": get_human_size(db_size + media_size)
        }
    }


@router.get("/git-status")
async def get_git_status():
    """
    è·å– Git ä»“åº“çŠ¶æ€

    æ£€æŸ¥å½“å‰åˆ†æ”¯ã€æ˜¯å¦æœ‰æ–°ç‰ˆæœ¬å¯ç”¨ç­‰
    """
    import subprocess

    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯ Git ä»“åº“
        if not os.path.exists(os.path.join(APP_DIR, ".git")):
            return {
                "is_git_repo": False,
                "message": "å½“å‰ä¸æ˜¯ Git ä»“åº“ï¼Œæ— æ³•ä½¿ç”¨è‡ªåŠ¨å‡çº§åŠŸèƒ½"
            }

        # é…ç½® git safe.directoryï¼ˆé¿å… dubious ownership é”™è¯¯ï¼‰
        subprocess.run(
            ["git", "config", "--global", "--add", "safe.directory", APP_DIR],
            capture_output=True,
            timeout=10
        )

        # è·å–å½“å‰åˆ†æ”¯
        branch_result = subprocess.run(
            ["git", "-C", APP_DIR, "branch", "--show-current"],
            capture_output=True,
            text=True,
            timeout=10
        )
        current_branch = branch_result.stdout.strip()

        # è·å–å½“å‰ commit
        commit_result = subprocess.run(
            ["git", "-C", APP_DIR, "rev-parse", "--short", "HEAD"],
            capture_output=True,
            text=True,
            timeout=10
        )
        current_commit = commit_result.stdout.strip()

        # è·å–è¿œç¨‹æ›´æ–°ï¼ˆä¸æ‹‰å–ï¼‰
        fetch_result = subprocess.run(
            ["git", "-C", APP_DIR, "fetch", "origin", current_branch],
            capture_output=True,
            text=True,
            timeout=30
        )

        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°ç‰ˆæœ¬
        status_result = subprocess.run(
            ["git", "-C", APP_DIR, "rev-list", "--count", f"HEAD..origin/{current_branch}"],
            capture_output=True,
            text=True,
            timeout=10
        )
        commits_behind = int(status_result.stdout.strip() or "0")

        # è·å–æœ€æ–°çš„è¿œç¨‹ commit
        remote_commit_result = subprocess.run(
            ["git", "-C", APP_DIR, "rev-parse", "--short", f"origin/{current_branch}"],
            capture_output=True,
            text=True,
            timeout=10
        )
        remote_commit = remote_commit_result.stdout.strip()

        return {
            "is_git_repo": True,
            "current_branch": current_branch,
            "current_commit": current_commit,
            "remote_commit": remote_commit,
            "commits_behind": commits_behind,
            "update_available": commits_behind > 0,
            "message": f"å‘ç° {commits_behind} ä¸ªæ–°ç‰ˆæœ¬" if commits_behind > 0 else "å·²æ˜¯æœ€æ–°ç‰ˆæœ¬"
        }

    except subprocess.TimeoutExpired:
        raise HTTPException(status_code=500, detail="Git æ“ä½œè¶…æ—¶")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å– Git çŠ¶æ€å¤±è´¥: {str(e)}")


class UpgradeRequest(BaseModel):
    """å‡çº§è¯·æ±‚"""
    auto_backup: bool = True  # å‡çº§å‰æ˜¯å¦è‡ªåŠ¨å¤‡ä»½
    skip_pip: bool = False    # æ˜¯å¦è·³è¿‡ pip install


@router.post("/upgrade")
async def upgrade_system(request: UpgradeRequest, background_tasks: BackgroundTasks):
    """
    ä¸€é”®å‡çº§ç³»ç»Ÿ

    æ‰§è¡Œæ­¥éª¤:
    1. æ£€æŸ¥ Git ä»“åº“çŠ¶æ€
    2. åˆ›å»ºè‡ªåŠ¨å¤‡ä»½ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    3. æ‹‰å–æœ€æ–°ä»£ç  (git pull)
    4. å®‰è£…ä¾èµ– (pip install)
    5. é‡å¯æœåŠ¡

    æ³¨æ„: æ­¤æ“ä½œä¼šé‡å¯æœåŠ¡,å¯¼è‡´çŸ­æš‚ä¸å¯ç”¨
    """
    import subprocess

    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯ Git ä»“åº“
        if not os.path.exists(os.path.join(APP_DIR, ".git")):
            raise HTTPException(
                status_code=400,
                detail="å½“å‰ä¸æ˜¯ Git ä»“åº“ï¼Œæ— æ³•ä½¿ç”¨è‡ªåŠ¨å‡çº§åŠŸèƒ½ã€‚è¯·ä½¿ç”¨æ‰‹åŠ¨éƒ¨ç½²æ–¹å¼ã€‚"
            )

        # é…ç½® git safe.directoryï¼ˆé¿å… dubious ownership é”™è¯¯ï¼‰
        subprocess.run(
            ["git", "config", "--global", "--add", "safe.directory", APP_DIR],
            capture_output=True,
            timeout=10
        )

        # è·å–å½“å‰åˆ†æ”¯
        branch_result = subprocess.run(
            ["git", "-C", APP_DIR, "branch", "--show-current"],
            capture_output=True,
            text=True,
            timeout=10
        )
        current_branch = branch_result.stdout.strip()

        # æ£€æŸ¥æ˜¯å¦æœ‰æœ¬åœ°æœªæäº¤çš„æ›´æ”¹
        status_result = subprocess.run(
            ["git", "-C", APP_DIR, "status", "--porcelain"],
            capture_output=True,
            text=True,
            timeout=10
        )

        if status_result.stdout.strip():
            raise HTTPException(
                status_code=400,
                detail="æ£€æµ‹åˆ°æœ¬åœ°æœ‰æœªæäº¤çš„æ›´æ”¹ï¼Œè¯·å…ˆå¤„ç†è¿™äº›æ›´æ”¹åå†å‡çº§"
            )

        upgrade_log = []

        # æ­¥éª¤ 1: è‡ªåŠ¨å¤‡ä»½
        if request.auto_backup:
            upgrade_log.append("ğŸ“¦ æ­£åœ¨åˆ›å»ºå‡çº§å‰å¤‡ä»½...")
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"EnglishLearn_å‡çº§å‰å¤‡ä»½_{timestamp}.tar.gz"
            filepath = os.path.join(BACKUP_DIR, filename)

            ensure_backup_dir()

            # åˆ›å»ºå¤‡ä»½ä¿¡æ¯
            backup_info = {
                "backup_time": datetime.datetime.now().isoformat(),
                "description": f"å‡çº§å‰è‡ªåŠ¨å¤‡ä»½ - {timestamp}",
                "version": "1.0",
                "db_included": True,
                "media_included": os.path.exists(MEDIA_DIR),
                "backup_type": "pre_upgrade",
                "restore_note": "æ­¤å¤‡ä»½åœ¨ç³»ç»Ÿå‡çº§å‰è‡ªåŠ¨åˆ›å»º"
            }

            with tarfile.open(filepath, "w:gz") as tar:
                # æ·»åŠ å¤‡ä»½ä¿¡æ¯
                import io
                info_json = json.dumps(backup_info, indent=2, ensure_ascii=False)
                info_bytes = info_json.encode('utf-8')
                info_tarinfo = tarfile.TarInfo(name='backup_info.json')
                info_tarinfo.size = len(info_bytes)
                info_tarinfo.mtime = datetime.datetime.now().timestamp()
                tar.addfile(info_tarinfo, io.BytesIO(info_bytes))

                if os.path.exists(DB_PATH):
                    tar.add(DB_PATH, arcname='el.db')
                if os.path.exists(MEDIA_DIR):
                    tar.add(MEDIA_DIR, arcname='media')

            upgrade_log.append(f"âœ… å¤‡ä»½å·²åˆ›å»º: {filename}")

        # æ­¥éª¤ 2: æ‹‰å–æœ€æ–°ä»£ç 
        upgrade_log.append("ğŸ“¥ æ­£åœ¨æ‹‰å–æœ€æ–°ä»£ç ...")
        pull_result = subprocess.run(
            ["git", "-C", APP_DIR, "pull", "--rebase", "origin", current_branch],
            capture_output=True,
            text=True,
            timeout=60
        )

        if pull_result.returncode != 0:
            raise HTTPException(
                status_code=500,
                detail=f"Git pull å¤±è´¥: {pull_result.stderr}"
            )

        upgrade_log.append("âœ… ä»£ç å·²æ›´æ–°")
        upgrade_log.append(pull_result.stdout)

        # æ­¥éª¤ 3: å®‰è£…ä¾èµ–
        if not request.skip_pip:
            upgrade_log.append("ğŸ“¦ æ­£åœ¨å®‰è£… Python ä¾èµ–...")

            # è·å– Python è·¯å¾„
            python_bin = os.path.join(APP_DIR, "venv", "bin", "python")
            if not os.path.exists(python_bin):
                python_bin = "python3"  # å¼€å‘ç¯å¢ƒ fallback

            pip_result = subprocess.run(
                [python_bin, "-m", "pip", "install", "-r",
                 os.path.join(APP_DIR, "backend", "requirements.txt")],
                capture_output=True,
                text=True,
                timeout=300
            )

            if pip_result.returncode != 0:
                upgrade_log.append(f"âš ï¸  ä¾èµ–å®‰è£…è­¦å‘Š: {pip_result.stderr}")
            else:
                upgrade_log.append("âœ… ä¾èµ–å·²æ›´æ–°")

        # æ­¥éª¤ 4: æ£€æŸ¥å¹¶åˆå¹¶ç¯å¢ƒé…ç½®
        upgrade_log.append("âš™ï¸  æ£€æŸ¥ç¯å¢ƒé…ç½®...")
        env_update_result = check_and_merge_env_config(APP_DIR)
        if env_update_result.get("new_keys"):
            upgrade_log.append(f"âœ… å·²æ·»åŠ  {len(env_update_result['new_keys'])} ä¸ªæ–°é…ç½®é¡¹")
            for key in env_update_result['new_keys'][:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                upgrade_log.append(f"   + {key}")
            if len(env_update_result['new_keys']) > 5:
                upgrade_log.append(f"   ... ç­‰ {len(env_update_result['new_keys']) - 5} ä¸ª")
        else:
            upgrade_log.append("âœ… ç¯å¢ƒé…ç½®å·²æ˜¯æœ€æ–°")

        # æ­¥éª¤ 5: æ‰§è¡Œæ•°æ®åº“è¿ç§»
        upgrade_log.append("ğŸ—„ï¸  æ£€æŸ¥æ•°æ®åº“è¿ç§»...")

        try:
            # å¯¼å…¥è¿ç§»ç®¡ç†å™¨
            sys.path.insert(0, os.path.join(APP_DIR, "backend"))
            from migration_manager import MigrationManager

            # åˆ›å»ºè¿ç§»ç®¡ç†å™¨å®ä¾‹
            manager = MigrationManager(DB_PATH)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿ç§»
            if manager.check_migration_needed():
                upgrade_log.append("ğŸ“‹ å‘ç°å¾…æ‰§è¡Œçš„æ•°æ®åº“è¿ç§»")

                # æ‰§è¡Œè¿ç§»
                migration_results = manager.migrate_all(stop_on_error=True)

                if migration_results['failed'] > 0:
                    raise Exception(
                        f"æ•°æ®åº“è¿ç§»å¤±è´¥: {migration_results['failed']} ä¸ªè¿ç§»å¤±è´¥"
                    )

                upgrade_log.append(
                    f"âœ… æ•°æ®åº“è¿ç§»å®Œæˆ: "
                    f"æˆåŠŸ {migration_results['success']} ä¸ª"
                )
            else:
                upgrade_log.append("âœ… æ•°æ®åº“å·²æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼Œæ— éœ€è¿ç§»")

        except ImportError:
            upgrade_log.append("âš ï¸  æœªæ‰¾åˆ°è¿ç§»ç®¡ç†å™¨ï¼Œè·³è¿‡æ•°æ®åº“è¿ç§»")
        except Exception as e:
            upgrade_log.append(f"âŒ æ•°æ®åº“è¿ç§»å¤±è´¥: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail=f"æ•°æ®åº“è¿ç§»å¤±è´¥: {str(e)}"
            )

        # æ­¥éª¤ 6: é‡å¯æœåŠ¡
        upgrade_log.append("ğŸ”„ å‡†å¤‡é‡å¯æœåŠ¡...")

        # æ£€æŸ¥æ˜¯å¦æ˜¯ systemd æœåŠ¡
        service_check = subprocess.run(
            ["systemctl", "is-active", "englishlearn"],
            capture_output=True,
            text=True
        )

        if service_check.returncode == 0:
            # ç”Ÿäº§ç¯å¢ƒ - ä½¿ç”¨ systemd
            subprocess.run(["systemctl", "restart", "englishlearn"], timeout=30)
            upgrade_log.append("âœ… æœåŠ¡å·²é‡å¯ (systemd)")
        else:
            # å¼€å‘ç¯å¢ƒ - æç¤ºæ‰‹åŠ¨é‡å¯
            upgrade_log.append("âš ï¸  è¯·æ‰‹åŠ¨é‡å¯å¼€å‘æœåŠ¡å™¨")

        upgrade_log.append("ğŸ‰ å‡çº§å®Œæˆï¼")

        return {
            "success": True,
            "message": "ç³»ç»Ÿå‡çº§æˆåŠŸ",
            "log": upgrade_log,
            "note": "å¦‚æœä½¿ç”¨å¼€å‘æœåŠ¡å™¨ï¼Œè¯·æ‰‹åŠ¨é‡å¯"
        }

    except subprocess.TimeoutExpired:
        raise HTTPException(status_code=500, detail="å‡çº§æ“ä½œè¶…æ—¶")
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å‡çº§å¤±è´¥: {str(e)}")
